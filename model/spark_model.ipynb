{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NQa86fUNjhs"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.sql.types import DateType\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"PricePrediction\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnZhCJYkNjh0",
        "outputId": "8fa23e5e-d244-4e9a-9fc0-77ff712b6ec9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: pyspark\n",
            "Version: 3.0.1\n",
            "Summary: Apache Spark Python API\n",
            "Home-page: https://github.com/apache/spark/tree/master/python\n",
            "Author: Spark Developers\n",
            "Author-email: dev@spark.apache.org\n",
            "License: http://www.apache.org/licenses/LICENSE-2.0\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: py4j\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKnwsY30Njh2",
        "outputId": "0b77250b-a5a9-4911-c7e0-8272af639055"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- host_id: string (nullable = true)\n",
            " |-- host_name: string (nullable = true)\n",
            " |-- neighbourhood_group: string (nullable = true)\n",
            " |-- neighbourhood: string (nullable = true)\n",
            " |-- latitude: string (nullable = true)\n",
            " |-- longitude: string (nullable = true)\n",
            " |-- room_type: string (nullable = true)\n",
            " |-- price: string (nullable = true)\n",
            " |-- minimum_nights: string (nullable = true)\n",
            " |-- number_of_reviews: string (nullable = true)\n",
            " |-- last_review: string (nullable = true)\n",
            " |-- reviews_per_month: string (nullable = true)\n",
            " |-- calculated_host_listings_count: string (nullable = true)\n",
            " |-- availability_365: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "df = spark.read.format('csv').option('header',True).option('multiLine', True).load('AB_NYC_2019.csv')\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJYC_486Njh4",
        "outputId": "a384754d-dfd8-4a00-a310-12f68d3f9cb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- neighbourhood_group: string (nullable = true)\n",
            " |-- neighbourhood: string (nullable = true)\n",
            " |-- latitude: string (nullable = true)\n",
            " |-- longitude: string (nullable = true)\n",
            " |-- room_type: string (nullable = true)\n",
            " |-- price: string (nullable = true)\n",
            " |-- minimum_nights: string (nullable = true)\n",
            " |-- number_of_reviews: string (nullable = true)\n",
            " |-- last_review: string (nullable = true)\n",
            " |-- reviews_per_month: string (nullable = true)\n",
            " |-- calculated_host_listings_count: string (nullable = true)\n",
            " |-- availability_365: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# drop redundant columns and filter outliers\n",
        "df = df.drop('id', 'host_name', 'host_id')\n",
        "df = df.filter(df.price<800)\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lJO2kcONjh5",
        "outputId": "88f450b2-e02c-4bcf-da60-67b950f3612f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- neighbourhood_group: string (nullable = true)\n",
            " |-- neighbourhood: string (nullable = true)\n",
            " |-- room_type: string (nullable = true)\n",
            " |-- latitude_num: float (nullable = true)\n",
            " |-- longitude_num: float (nullable = true)\n",
            " |-- number_of_reviews_int: integer (nullable = true)\n",
            " |-- minimum_nights_int: integer (nullable = true)\n",
            " |-- reviews_per_month_num: float (nullable = true)\n",
            " |-- calculated_host_listings_count_num: integer (nullable = true)\n",
            " |-- availability_365_int: integer (nullable = true)\n",
            " |-- last_review_date: date (nullable = true)\n",
            " |-- price_num: float (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# After data loading all columns have string dtype. Transform to correct types\n",
        "from pyspark.sql.types import DateType\n",
        "df = df.withColumn('latitude_num', df['latitude'].cast('float')).drop('latitude') \\\n",
        "    .withColumn('longitude_num', df['longitude'].cast('float')).drop('longitude') \\\n",
        "    .withColumn('number_of_reviews_int', df['number_of_reviews'].cast('int')).drop('number_of_reviews') \\\n",
        "    .withColumn('minimum_nights_int', df['minimum_nights'].cast('int')).drop('minimum_nights') \\\n",
        "    .withColumn('reviews_per_month_num', df['reviews_per_month'].cast('float')).drop('reviews_per_month') \\\n",
        "    .withColumn('calculated_host_listings_count_num', df['calculated_host_listings_count'].cast('int')).drop('calculated_host_listings_count') \\\n",
        "    .withColumn('availability_365_int', df['availability_365'].cast('int')).drop('availability_365') \\\n",
        "    .withColumn('last_review_date', df['last_review'].cast(DateType())).drop('last_review') \\\n",
        "    .withColumn('price_num', df['price'].cast('float')).drop('price')\n",
        "#.withColumn(\"host_id_int\", df['host_id'].cast('int')).drop('host_id') \n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVqpGMH-Njh6"
      },
      "outputs": [],
      "source": [
        "# Categorize string categorical columns into integers\n",
        "from pyspark.ml.feature import StringIndexer, StringIndexerModel\n",
        "stringIndexer = StringIndexer(inputCols=[\"neighbourhood_group\", 'neighbourhood', 'room_type'],\n",
        "                              outputCols=[\"neighbourhood_group_int\", 'neighbourhood_int', 'room_type_int'],\n",
        "                              stringOrderType=\"frequencyDesc\")\n",
        "stringIndexer_model = stringIndexer.fit(df)\n",
        "df = stringIndexer_model.transform(df)\n",
        "df = df.drop(*[\"neighbourhood_group\", 'neighbourhood', 'room_type'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg2B1wM8Njh8"
      },
      "outputs": [],
      "source": [
        "# stringIndexerPath = './stringIndexer'\n",
        "# stringIndexer_model.save(stringIndexerPath)\n",
        "# stringIndexer_model = StringIndexerModel.load(stringIndexerPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRpm5m91Njh9"
      },
      "outputs": [],
      "source": [
        "# One-hot encode them\n",
        "from pyspark.ml.feature import OneHotEncoder, OneHotEncoderModel\n",
        "ohe = OneHotEncoder(inputCols=[\"neighbourhood_group_int\", 'neighbourhood_int', 'room_type_int'],\n",
        "                    outputCols=[\"neighbourhood_group_vec\", 'neighbourhood_vec', 'room_type_vec'])\n",
        "ohe_model = ohe.fit(df)\n",
        "encoded = ohe_model.transform(df)\n",
        "df = encoded.drop(*[\"neighbourhood_group_int\", 'neighbourhood_int', 'room_type_int'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8cXJRUpNjh_"
      },
      "outputs": [],
      "source": [
        "# ohe_model_path = './ohe'\n",
        "# ohe_model.save(ohe_model_path)\n",
        "# ohe_model = OneHotEncoderModel.load(ohe_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlXDOb95NjiB"
      },
      "outputs": [],
      "source": [
        "# Some feature ingineering\n",
        "from pyspark.sql.functions import datediff, to_date, lit, length, when\n",
        "df = df.withColumn(\"minimum_nights\", when(df[\"minimum_nights_int\"] > 30, 30).otherwise(df[\"minimum_nights_int\"])).drop('minimum_nights_int')\n",
        "df = df.withColumn('name_length', length('name')).drop('name')\n",
        "df = df.withColumn(\"days_from_review\", \n",
        "                   datediff(to_date(lit(\"2020-01-01\")),\n",
        "                            to_date(\"last_review_date\",\"yyyy-MM-dd\"))).drop('last_review_date')\n",
        "# Fill NaNs and Nulls\n",
        "df = df.fillna({'days_from_review': 0,\n",
        "                'reviews_per_month_num': 0,\n",
        "                'name_length': 0,\n",
        "                'calculated_host_listings_count_num': 1,\n",
        "                'number_of_reviews_int': 0 })\n",
        "df = df.na.drop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syiSU337NjiC"
      },
      "outputs": [],
      "source": [
        "# df = df.drop(\"latitude_num\",'longitude_num')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3UHk_lMNjiD"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfJTzxTsNjiE"
      },
      "outputs": [],
      "source": [
        "# from pyspark.sql.functions import isnan, when, count, col, isnull\n",
        "\n",
        "# print(df.agg({\"minimum_nights\": \"max\"}).collect()[0][0])\n",
        "# df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tn0e9WEjNjiE"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNoT4M5PNjiF"
      },
      "outputs": [],
      "source": [
        "# create feature column for spark regressor\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from copy import copy\n",
        "inputCols = copy(df.columns)\n",
        "inputCols.remove('price_num')\n",
        "vectorAssembler = VectorAssembler(inputCols = inputCols,\n",
        "                                  outputCol = 'features',\n",
        "                                 )\n",
        "df = vectorAssembler.transform(df)\n",
        "df = df.select(['features', 'price_num'])\n",
        "df.show(5,False)\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfA3Za3CNjiF"
      },
      "outputs": [],
      "source": [
        "vectorAssembler_path = './vectorAssembler'\n",
        "vectorAssembler.save(vectorAssembler_path)\n",
        "vectorAssembler = vectorAssembler.load(vectorAssembler_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JMZ_2Q6NjiG",
        "outputId": "7887a27e-c061-4d5a-c3b5-77fffb55f5e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|features_vec                                                                                                                                 |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|(237,[0,1,2,3,4,5,7,61,232,234,235,236],[40.647491455078125,-73.97236633300781,9.0,0.20999999344348907,5.0,365.0,1.0,1.0,1.0,0.0,34.0,439.0])|\n",
            "|(237,[0,1,2,3,4,5,6,20,231,234,235,236],[40.75362014770508,-73.98377227783203,45.0,0.3799999952316284,1.0,355.0,1.0,1.0,1.0,0.0,21.0,225.0]) |\n",
            "|(237,[0,1,4,5,6,13,232,234,235],[40.80902099609375,-73.94190216064453,0.0,365.0,1.0,1.0,1.0,2.0,35.0])                                       |\n",
            "|(237,[0,1,2,3,4,5,7,30,231,234,235,236],[40.68513870239258,-73.95976257324219,270.0,4.639999866485596,0.0,194.0,1.0,1.0,1.0,0.0,31.0,180.0]) |\n",
            "|(237,[0,1,2,3,4,6,21,231,234,235,236],[40.79851150512695,-73.9439926147461,9.0,0.10000000149011612,0.0,1.0,1.0,1.0,9.0,48.0,408.0])          |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Vectorize feature column\n",
        "from pyspark.ml.feature import VectorIndexer, VectorIndexerModel\n",
        "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"features_vec\", maxCategories=230).fit(df)\n",
        "df = featureIndexer.transform(df)\n",
        "df = df.select(['features_vec', 'price_num'])\n",
        "df.select('features_vec').show(5, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzqrZJiXNjiO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copy of model.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}