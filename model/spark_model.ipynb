{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"PricePrediction\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 3.0.1\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: /usr/local/lib/python3.7/dist-packages\n",
      "Requires: py4j\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- host_id: string (nullable = true)\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- neighbourhood_group: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- minimum_nights: string (nullable = true)\n",
      " |-- number_of_reviews: string (nullable = true)\n",
      " |-- last_review: string (nullable = true)\n",
      " |-- reviews_per_month: string (nullable = true)\n",
      " |-- calculated_host_listings_count: string (nullable = true)\n",
      " |-- availability_365: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "df = spark.read.format('csv').option('header',True).option('multiLine', True).load('AB_NYC_2019.csv')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- neighbourhood_group: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- minimum_nights: string (nullable = true)\n",
      " |-- number_of_reviews: string (nullable = true)\n",
      " |-- last_review: string (nullable = true)\n",
      " |-- reviews_per_month: string (nullable = true)\n",
      " |-- calculated_host_listings_count: string (nullable = true)\n",
      " |-- availability_365: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop redundant columns and filter outliers\n",
    "df = df.drop('id', 'host_name', 'host_id')\n",
    "df = df.filter(df.price<800)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- neighbourhood_group: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- latitude_num: float (nullable = true)\n",
      " |-- longitude_num: float (nullable = true)\n",
      " |-- number_of_reviews_int: integer (nullable = true)\n",
      " |-- minimum_nights_int: integer (nullable = true)\n",
      " |-- reviews_per_month_num: float (nullable = true)\n",
      " |-- calculated_host_listings_count_num: integer (nullable = true)\n",
      " |-- availability_365_int: integer (nullable = true)\n",
      " |-- last_review_date: date (nullable = true)\n",
      " |-- price_num: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# After data loading all columns have string dtype. Transform to correct types\n",
    "from pyspark.sql.types import DateType\n",
    "df = df.withColumn('latitude_num', df['latitude'].cast('float')).drop('latitude') \\\n",
    "    .withColumn('longitude_num', df['longitude'].cast('float')).drop('longitude') \\\n",
    "    .withColumn('number_of_reviews_int', df['number_of_reviews'].cast('int')).drop('number_of_reviews') \\\n",
    "    .withColumn('minimum_nights_int', df['minimum_nights'].cast('int')).drop('minimum_nights') \\\n",
    "    .withColumn('reviews_per_month_num', df['reviews_per_month'].cast('float')).drop('reviews_per_month') \\\n",
    "    .withColumn('calculated_host_listings_count_num', df['calculated_host_listings_count'].cast('int')).drop('calculated_host_listings_count') \\\n",
    "    .withColumn('availability_365_int', df['availability_365'].cast('int')).drop('availability_365') \\\n",
    "    .withColumn('last_review_date', df['last_review'].cast(DateType())).drop('last_review') \\\n",
    "    .withColumn('price_num', df['price'].cast('float')).drop('price')\n",
    "#.withColumn(\"host_id_int\", df['host_id'].cast('int')).drop('host_id') \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize string categorical columns into integers\n",
    "from pyspark.ml.feature import StringIndexer, StringIndexerModel\n",
    "stringIndexer = StringIndexer(inputCols=[\"neighbourhood_group\", 'neighbourhood', 'room_type'],\n",
    "                              outputCols=[\"neighbourhood_group_int\", 'neighbourhood_int', 'room_type_int'],\n",
    "                              stringOrderType=\"frequencyDesc\")\n",
    "stringIndexer_model = stringIndexer.fit(df)\n",
    "df = stringIndexer_model.transform(df)\n",
    "df = df.drop(*[\"neighbourhood_group\", 'neighbourhood', 'room_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o158.save.\n: java.io.IOException: Path ./stringIndexer already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-385e5bbe09a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstringIndexerPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./stringIndexer'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstringIndexer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstringIndexerPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstringIndexer_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIndexerModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstringIndexerPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;34m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path should be a basestring, got type %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o158.save.\n: java.io.IOException: Path ./stringIndexer already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "# stringIndexerPath = './stringIndexer'\n",
    "# stringIndexer_model.save(stringIndexerPath)\n",
    "# stringIndexer_model = StringIndexerModel.load(stringIndexerPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode them\n",
    "from pyspark.ml.feature import OneHotEncoder, OneHotEncoderModel\n",
    "ohe = OneHotEncoder(inputCols=[\"neighbourhood_group_int\", 'neighbourhood_int', 'room_type_int'],\n",
    "                    outputCols=[\"neighbourhood_group_vec\", 'neighbourhood_vec', 'room_type_vec'])\n",
    "ohe_model = ohe.fit(df)\n",
    "encoded = ohe_model.transform(df)\n",
    "df = encoded.drop(*[\"neighbourhood_group_int\", 'neighbourhood_int', 'room_type_int'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ohe_model_path = './ohe'\n",
    "# ohe_model.save(ohe_model_path)\n",
    "# ohe_model = OneHotEncoderModel.load(ohe_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some feature ingineering\n",
    "from pyspark.sql.functions import datediff, to_date, lit, length, when\n",
    "df = df.withColumn(\"minimum_nights\", when(df[\"minimum_nights_int\"] > 30, 30).otherwise(df[\"minimum_nights_int\"])).drop('minimum_nights_int')\n",
    "df = df.withColumn('name_length', length('name')).drop('name')\n",
    "df = df.withColumn(\"days_from_review\", \n",
    "                   datediff(to_date(lit(\"2020-01-01\")),\n",
    "                            to_date(\"last_review_date\",\"yyyy-MM-dd\"))).drop('last_review_date')\n",
    "# Fill NaNs and Nulls\n",
    "df = df.fillna({'days_from_review': 0,\n",
    "                'reviews_per_month_num': 0,\n",
    "                'name_length': 0,\n",
    "                'calculated_host_listings_count_num': 1,\n",
    "                'number_of_reviews_int': 0 })\n",
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(\"latitude_num\",'longitude_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import isnan, when, count, col, isnull\n",
    "\n",
    "# print(df.agg({\"minimum_nights\": \"max\"}).collect()[0][0])\n",
    "# df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Data type string of column name is not supported.\nData type date of column last_review_date is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-808f83e219cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                   \u001b[0moutputCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'features'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                  )\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorAssembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'price_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Data type string of column name is not supported.\nData type date of column last_review_date is not supported."
     ]
    }
   ],
   "source": [
    "# create feature column for spark regressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from copy import copy\n",
    "inputCols = copy(df.columns)\n",
    "inputCols.remove('price_num')\n",
    "vectorAssembler = VectorAssembler(inputCols = inputCols,\n",
    "                                  outputCol = 'features',\n",
    "                                 )\n",
    "df = vectorAssembler.transform(df)\n",
    "df = df.select(['features', 'price_num'])\n",
    "df.show(5,False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler_path = './vectorAssembler'\n",
    "vectorAssembler.save(vectorAssembler_path)\n",
    "vectorAssembler = vectorAssembler.load(vectorAssembler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features_vec                                                                                                                                 |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(237,[0,1,2,3,4,5,7,61,232,234,235,236],[40.647491455078125,-73.97236633300781,9.0,0.20999999344348907,5.0,365.0,1.0,1.0,1.0,0.0,34.0,439.0])|\n",
      "|(237,[0,1,2,3,4,5,6,20,231,234,235,236],[40.75362014770508,-73.98377227783203,45.0,0.3799999952316284,1.0,355.0,1.0,1.0,1.0,0.0,21.0,225.0]) |\n",
      "|(237,[0,1,4,5,6,13,232,234,235],[40.80902099609375,-73.94190216064453,0.0,365.0,1.0,1.0,1.0,2.0,35.0])                                       |\n",
      "|(237,[0,1,2,3,4,5,7,30,231,234,235,236],[40.68513870239258,-73.95976257324219,270.0,4.639999866485596,0.0,194.0,1.0,1.0,1.0,0.0,31.0,180.0]) |\n",
      "|(237,[0,1,2,3,4,6,21,231,234,235,236],[40.79851150512695,-73.9439926147461,9.0,0.10000000149011612,0.0,1.0,1.0,1.0,9.0,48.0,408.0])          |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vectorize feature column\n",
    "from pyspark.ml.feature import VectorIndexer, VectorIndexerModel\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"features_vec\", maxCategories=230).fit(df)\n",
    "df = featureIndexer.transform(df)\n",
    "df = df.select(['features_vec', 'price_num'])\n",
    "df.select('features_vec').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test \n",
    "splits = df.randomSplit([0.8, 0.2])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 79.387118\n",
      "r2: 0.421572\n"
     ]
    }
   ],
   "source": [
    "# fit linear regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features_vec', labelCol='price_num')\n",
    "lr_model = lr.fit(train_df)\n",
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+--------------------+\n",
      "|        prediction|price_num|        features_vec|\n",
      "+------------------+---------+--------------------+\n",
      "|177.57065077970765|    149.0|(237,[0,1,2,3,4,5...|\n",
      "| 216.3503272705202|    300.0|(237,[0,1,2,3,4,5...|\n",
      "|200.38307160902332|    104.0|(237,[0,1,2,3,4,5...|\n",
      "|166.23831671215885|    299.0|(237,[0,1,2,3,4,5...|\n",
      "|171.80952899411204|    200.0|(237,[0,1,2,3,4,5...|\n",
      "+------------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "R Squared (R2) on test data = 0.410451\n"
     ]
    }
   ],
   "source": [
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.select(\"prediction\",\"price_num\",\"features_vec\").show(5)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"price_num\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/opt/workspace/lr_model'\n",
    "lr_model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "rfc = RandomForestRegressor(featuresCol = 'features_vec', labelCol='price_num', #numTrees=20,\n",
    "                           maxDepth=30, maxBins=250)\n",
    "rfc_model = rfc.fit(train_df)\n",
    "rfc_predictions = rfc_model.transform(test_df)\n",
    "rfc_predictions.select(\"prediction\",\"price_num\",\"features_vec\").show(5)\n",
    "rfc_evaluator = RegressionEvaluator(labelCol=\"price_num\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % rfc_evaluator.evaluate(rfc_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.regression import RandomForestRegressionModel, LinearRegressionModel\n",
    "# model_path = '/opt/workspace/lr_model'\n",
    "# lr_model.save(model_path)\n",
    "# model2 = LinearRegressionModel.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf stringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|        prediction|\n",
      "+------------------+\n",
      "| 158.4032963124846|\n",
      "| 165.0361241964565|\n",
      "|189.95015759149828|\n",
      "|150.68205976478203|\n",
      "|198.97464480284543|\n",
      "+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = '/opt/workspace/lr_model'\n",
    "lr_model = LinearRegressionModel.load(model_path)\n",
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.select(\"prediction\").show(5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([])\n",
    "request = sqlContext.createDataFrame(sc.emptyRDD(), schema)\n",
    "request = request\\\n",
    ".withColumn(\"category\",F.lit('nation'))\\\n",
    ".withColumn(\"category_id\",F.lit('nation'))\\\n",
    ".withColumn(\"bucket\",F.lit(bucket))\\\n",
    ".withColumn(\"prop_count\",F.lit(prop_count))\\\n",
    ".withColumn(\"event_count\",F.lit(event_count))\\\n",
    ".withColumn(\"accum_prop_count\",F.lit(accum_prop_count))\\\n",
    ".withColumn(\"accum_event_count\",F.lit(accum_event_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderModel, VectorAssembler, StringIndexerModel, VectorIndexerModel\n",
    "vectorAssembler_path = './vectorAssembler'\n",
    "ohe_model_path = './ohe'\n",
    "stringIndexerPath = './stringIndexer'\n",
    "featureIndexer_path = './featureIndexer'\n",
    "\n",
    "feature_model = VectorIndexerModel.load(featureIndexer_path)\n",
    "vectorAssembler = vectorAssembler.load(vectorAssembler_path)\n",
    "ohe_model = OneHotEncoderModel.load(ohe_model_path)\n",
    "stringIndexer_model = StringIndexerModel.load(stringIndexerPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = stringIndexer_model.transform(df)\n",
    "df = df.drop(*[\"neighbourhood_group\", 'neighbourhood', 'room_type'])\n",
    "df = ohe_model.transform(df)\n",
    "df = df.drop(*[\"neighbourhood_group_int\", 'neighbourhood_int', 'room_type_int'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, to_date, lit, length, when\n",
    "df = df.withColumn(\"minimum_nights\", when(df[\"minimum_nights_int\"] > 30, 30).otherwise(df[\"minimum_nights_int\"])).drop('minimum_nights_int')\n",
    "df = df.withColumn('name_length', length('name')).drop('name')\n",
    "df = df.fillna({'days_from_review': 0,\n",
    "                'reviews_per_month_num': 0,\n",
    "                'name_length': 0,\n",
    "                'calculated_host_listings_count_num': 1,\n",
    "                'number_of_reviews_int': 0 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = vectorAssembler.transform(df)\n",
    "df = df.select(['features', 'price_num'])\n",
    "df = featureIndexer.transform(df)\n",
    "df = df.select(['features_vec', 'price_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.0.*\n",
      "  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 204.2 MB 27 kB/s s eta 0:00:01                    | 8.4 MB 12.9 MB/s eta 0:00:1614.8 MB 12.9 MB/s eta 0:00:15█▊                             | 17.7 MB 12.9 MB/s eta 0:00:15 MB 12.9 MB/s eta 0:00:15 MB 12.9 MB/s eta 0:00:14                       | 26.6 MB 12.9 MB/s eta 0:00:14    | 30.3 MB 12.9 MB/s eta 0:00:14         | 34.1 MB 12.9 MB/s eta 0:00:14                        | 36.8 MB 12.9 MB/s eta 0:00:1300:13 eta 0:00:13                | 49.5 MB 12.9 MB/s eta 0:00:12ta 0:00:12ta 0:00:12[K     |█████████▍                      | 59.7 MB 60.2 MB/s eta 0:00:03              | 63.5 MB 60.2 MB/s eta 0:00:03    |██████████▋                     | 67.9 MB 60.2 MB/s eta 0:00:03 MB 60.2 MB/s eta 0:00:03      | 75.2 MB 60.2 MB/s eta 0:00:03██▍                   | 78.8 MB 60.2 MB/s eta 0:00:03.2 MB/s eta 0:00:030.2 MB/s eta 0:00:02�██████████                  | 89.9 MB 60.2 MB/s eta 0:00:02|██████████████▋                 | 93.1 MB 60.2 MB/s eta 0:00:02        | 96.6 MB 60.2 MB/s eta 0:00:02�█████████████▋                | 99.7 MB 60.2 MB/s eta 0:00:02      | 102.9 MB 60.2 MB/s eta 0:00:02 |████████████████▊               | 106.4 MB 60.2 MB/s eta 0:00:02110.9 MB 60.2 MB/s eta 0:00:02ta 0:00:02████▏             | 116.2 MB 41.6 MB/s eta 0:00:03MB/s eta 0:00:03�██████             | 121.3 MB 41.6 MB/s eta 0:00:02 41.6 MB/s eta 0:00:02��████▌           | 130.7 MB 41.6 MB/s eta 0:00:02��████████████████▉           | 133.3 MB 41.6 MB/s eta 0:00:02137.1 MB 41.6 MB/s eta 0:00:02MB 41.6 MB/s eta 0:00:02        | 146.2 MB 41.6 MB/s eta 0:00:020:02�███████████        | 153.5 MB 41.6 MB/s eta 0:00:02██████████████████████▋       | 157.4 MB 41.6 MB/s eta 0:00:026 MB/s eta 0:00:02��███████▋      | 163.8 MB 41.6 MB/s eta 0:00:01�█████████▎     | 167.5 MB 41.6 MB/s eta 0:00:01��██▊     | 170.7 MB 41.6 MB/s eta 0:00:01ta 0:00:01��████████████████████████▋   | 182.5 MB 52.4 MB/s eta 0:00:01�████████████   | 184.8 MB 52.4 MB/s eta 0:00:01ta 0:00:01ta 0:00:01��██████████████▏ | 192.6 MB 52.4 MB/s eta 0:00:01��██████████████▊ | 195.8 MB 52.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting py4j==0.10.9\n",
      "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 48.0 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612245 sha256=d3642ba22c75c5c620f281dd30e2423c1b59212fe43ff12d521a116586fcee32\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/34/fa/b37b5cef503fc5148b478b2495043ba61b079120b7ff379f9b\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.7\n",
      "    Uninstalling py4j-0.10.7:\n",
      "      Successfully uninstalled py4j-0.10.7\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 2.4.4\n",
      "    Uninstalling pyspark-2.4.4:\n",
      "      Successfully uninstalled pyspark-2.4.4\n",
      "Successfully installed py4j-0.10.9 pyspark-3.0.1\n",
      "\u001b[33mWARNING: You are using pip version 20.3; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.0.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
